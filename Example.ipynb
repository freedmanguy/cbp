{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import catboost\n",
    "import gensim.models\n",
    "import gap_statistic\n",
    "import nltk\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as wv\n",
    "from gensim import utils\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import download\n",
    "from nltk.cluster import KMeansClusterer\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class for preprocessing dta - stemming, tokenizing, removing stop words\n",
    "porter = PorterStemmer()\n",
    "\n",
    "class PrepareSentance():\n",
    "    def __init__(self, df, text_column):\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.processed_df = []\n",
    "        \n",
    "    def tokenize(self, stem=True, remove_stopwords=True):\n",
    "        df = self.df.copy()\n",
    "        text_column = self.text_column\n",
    "        processed_df = [utils.simple_preprocess(t) for t in df[text_column]]\n",
    "        if remove_stopwords:\n",
    "            stop_words = set(stopwords.words('english')) \n",
    "            for i in range(len(processed_df)):\n",
    "                processed_df[i] = [w for w in processed_df[i] if not w in stop_words]\n",
    "        if stem:\n",
    "            for i in range(len(processed_df)):\n",
    "                processed_df[i] = [porter.stem(p) for p in processed_df[i]]\n",
    "        self.processed_df = processed_df\n",
    "        return processed_df\n",
    "      \n",
    "\n",
    "# classes for preprocessing training/test sets\n",
    "class TabularDescription():\n",
    "    def __init__(self, dataset, text_column, word_clusters, single_words=None):\n",
    "        self.set = dataset\n",
    "        self.text_column = text_column\n",
    "        self.word_clusters = word_clusters\n",
    "        self.single_words = single_words\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.training_features = None\n",
    "        if 'congress_gov_major_topic' in list(dataset.columns):\n",
    "            congress_subject_area = dataset[['billid','congress_gov_major_topic']]\n",
    "            congress_subject_area = congress_subject_area.set_index(keys='billid')\n",
    "            self.congress_subject_area = congress_subject_area\n",
    "        else:\n",
    "            self.congress_subject_area = None\n",
    "        \n",
    "    def get_dataset(self):\n",
    "        return self.set\n",
    "    \n",
    "    def get_text_column(self):\n",
    "        return self.text_column\n",
    "    \n",
    "    def get_word_clusters(self):\n",
    "        return self.word_clusters\n",
    "    \n",
    "    def get_single_words(self):\n",
    "        return self.single_words\n",
    "    \n",
    "    def get_congress_subject_area(self):\n",
    "        return self.congress_subject_area\n",
    "    \n",
    "    def get_training_features(self):\n",
    "        tsf = copy.deepcopy(self.training_features)\n",
    "        return tsf\n",
    "      \n",
    "        \n",
    "class TabularDescriptionTrain(TabularDescription):\n",
    "    def __init__(self, dataset, text_column, word_clusters, single_words=None):\n",
    "        super().__init__(dataset, text_column, word_clusters, single_words=single_words)\n",
    "    \n",
    "    def prepare_set_for_training(self, stem=True, remove_stopwords=True):\n",
    "        ts = self.get_dataset()\n",
    "        tc = self.get_text_column()\n",
    "        ts_pp = PrepareSentance(df=ts, text_column=tc)\n",
    "        ts_t = ts_pp.tokenize(stem=stem, remove_stopwords=remove_stopwords)\n",
    "        \n",
    "        billid = [[ts.billid[b]] * len(ts_pp.processed_df[b]) for b in range(len(ts_pp.processed_df))]\n",
    "        ts_train = pd.DataFrame({\n",
    "            'billid' : [item for bill in billid for item in bill], \n",
    "            'term' : [term for title in ts_pp.processed_df for term in title]\n",
    "        })\n",
    "            \n",
    "        word_clusters = self.get_word_clusters()    \n",
    "        ts_train = ts_train.merge(right=word_clusters, how='left')\n",
    "        ts_train = ts_train.astype(str)\n",
    "        ts_train['cluster_name'] = 'c_'\n",
    "        ts_train.cluster_name = ts_train.cluster_name.str.cat(ts_train.cluster)\n",
    "        \n",
    "        ts_dtm = ts_train.groupby(['billid', 'cluster_name']).size().reset_index()\n",
    "        ts_dtm = ts_dtm.rename(columns={0:'n'})\n",
    "        ts_dtm = ts_dtm.pivot(index=\"billid\", columns=\"cluster_name\", values=\"n\").fillna(0)\n",
    "        ts_dtm = ts_dtm.drop(labels='c_nan', axis=1)\n",
    "        \n",
    "        sw = self.get_single_words()                \n",
    "        if sw is not None:\n",
    "            ts_sw = ts_train.merge(right=sw, how='inner')\n",
    "            ts_sw = pd.DataFrame(ts_sw.groupby(['billid', 'term']).size())\n",
    "            ts_sw = ts_sw.reset_index()\n",
    "            ts_sw.columns = ['billid', 'term', 'n']\n",
    "            ts_sw = ts_sw.pivot(index=\"billid\", columns=\"term\", values=\"n\")\n",
    "            ts_dtm = ts_dtm.merge(right=ts_sw, left_index=True, right_index=True, how='left').fillna(0)\n",
    "            \n",
    "        sa = self.get_congress_subject_area()\n",
    "        if sa is not None:\n",
    "            ts_dtm = ts_dtm.merge(right=sa, left_index=True, right_index=True, how='left')\n",
    "        \n",
    "        y = pd.DataFrame({'billid':ts_dtm.index}).merge(right=ts[['billid', 'minor']], \n",
    "                                                        how='left', on='billid')\n",
    "        y = y.astype('str')\n",
    "        y = list(y['minor'])\n",
    "        \n",
    "        self.x = ts_dtm\n",
    "        self.y = y\n",
    "        self.training_features = list(ts_dtm.columns)\n",
    "        return ts_dtm, y\n",
    "      \n",
    "    \n",
    "class TabularDescriptionTest(TabularDescription):\n",
    "    def __init__(self, dataset, text_column, word_clusters, training_features, single_words=None):\n",
    "        super().__init__(dataset, text_column, word_clusters, single_words=single_words)\n",
    "        self.training_features = training_features\n",
    "        \n",
    "    def prepare_set_for_evaluation(self, stem=True, remove_stopwords=True):\n",
    "        ts = self.get_dataset()\n",
    "        tc = self.get_text_column()\n",
    "        ts_pp = PrepareSentance(df=ts, text_column=tc)\n",
    "        ts_t = ts_pp.tokenize(stem=stem, remove_stopwords=remove_stopwords)\n",
    "        \n",
    "        billid = [[ts.billid[b]] * len(ts_pp.processed_df[b]) for b in range(len(ts_pp.processed_df))]\n",
    "        ts_train = pd.DataFrame({\n",
    "            'billid' : [item for bill in billid for item in bill], \n",
    "            'term' : [term for title in ts_pp.processed_df for term in title]\n",
    "        })\n",
    "            \n",
    "        word_clusters = self.get_word_clusters()    \n",
    "        ts_train = ts_train.merge(right=word_clusters, how='left')\n",
    "        ts_train = ts_train.astype(str)\n",
    "        ts_train['cluster_name'] = 'c_'\n",
    "        ts_train.cluster_name = ts_train.cluster_name.str.cat(ts_train.cluster)\n",
    "        \n",
    "        ts_dtm = ts_train.groupby(['billid', 'cluster_name']).size().reset_index()\n",
    "        ts_dtm = ts_dtm.rename(columns={0:'n'})\n",
    "        ts_dtm = ts_dtm.pivot(index=\"billid\", columns=\"cluster_name\", values=\"n\").fillna(0)\n",
    "        ts_dtm = ts_dtm.drop(labels='c_nan', axis=1)\n",
    "        \n",
    "        sw = self.get_single_words()                \n",
    "        if sw is not None:\n",
    "            ts_sw = ts_train.merge(right=sw, how='inner')\n",
    "            ts_sw = pd.DataFrame(ts_sw.groupby(['billid', 'term']).size())\n",
    "            ts_sw = ts_sw.reset_index()\n",
    "            ts_sw.columns = ['billid', 'term', 'n']\n",
    "            ts_sw = ts_sw.pivot(index=\"billid\", columns=\"term\", values=\"n\")\n",
    "            ts_dtm = ts_dtm.merge(right=ts_sw, left_index=True, right_index=True, how='left').fillna(0)\n",
    "        \n",
    "        ts_columns = set(ts_dtm.columns)\n",
    "        training_features = set(self.get_training_features())\n",
    "        missing_features = training_features.difference(ts_columns)\n",
    "        number_of_columns = len(missing_features)\n",
    "        if number_of_columns > 0:\n",
    "            number_of_rows = len(ts_dtm)\n",
    "            missing_features_a = np.zeros((number_of_rows, number_of_columns))\n",
    "            missing_features_df = pd.DataFrame(missing_features_a)\n",
    "            missing_features_df.columns = missing_features\n",
    "            missing_features_df.index = ts_dtm.index\n",
    "            ts_dtm = pd.concat([ts_dtm, missing_features_df], axis = 1)\n",
    "        \n",
    "        columns_to_keep = copy.deepcopy(self.get_training_features())\n",
    "        sa = self.get_congress_subject_area()\n",
    "        if sa is not None:\n",
    "            columns_to_keep.remove('congress_gov_major_topic')\n",
    "        ts_dtm = ts_dtm[columns_to_keep]\n",
    "        \n",
    "        if sa is not None:\n",
    "            ts_dtm = ts_dtm.merge(right=sa, left_index=True, right_index=True, how='left')\n",
    "        \n",
    "        y = pd.DataFrame({'billid':ts_dtm.index}).merge(right=ts[['billid', 'minor']], \n",
    "                                                        how='left', on='billid')\n",
    "        y = y.astype('str')\n",
    "        y = list(y['minor'])\n",
    "        \n",
    "        self.x = ts_dtm\n",
    "        self.y = y\n",
    "        return ts_dtm, y\n",
    "    \n",
    "    \n",
    "# function for evaluating model performance    \n",
    "def get_classification_results(cbm_model, x, y=None):\n",
    "    pred_prob_set = pd.DataFrame(cbm_model.predict_proba(x))\n",
    "    pred_prob_set.columns = cbm_model.classes_\n",
    "    pred_prob_set['probability'] = pred_prob_set.max(1)\n",
    "    pred_prob_set['predicted'] = cbm_model.predict(x)\n",
    "    if y is not None:\n",
    "        pred_prob_set['observed'] = y\n",
    "        pred_prob_set['match'] = [True if pred_prob_set['observed'][i] == pred_prob_set['predicted'][i] else False for i in pred_prob_set.index]\n",
    "    pred_prob_set['billid'] = list(x.index)\n",
    "    return pred_prob_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2dece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (prepared in advance)\n",
    "population_csv_file = 'population_93_114.csv'\n",
    "training_csv_file = 'training_80.csv'\n",
    "evaluation_csv_file = 'evaluation_set_80.csv'\n",
    "test_csv_file = 'test_set_80.csv'\n",
    "# data\n",
    "population = pd.read_csv(population_csv_file)\n",
    "training_set = pd.read_csv(training_csv_file)\n",
    "evaluation_set = pd.read_csv(evaluation_csv_file)\n",
    "test_set = pd.read_csv(test_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d38c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectors on population data\n",
    "population = population.query('congress > 107')\n",
    "population['title'] = [re.sub('united states code|other purposes', '', x, flags=re.IGNORECASE) for x in population['title']]\n",
    "\n",
    "population_pp = PrepareSentance(df=population, text_column='title')\n",
    "population_t = population_pp.tokenize()\n",
    "for i in population_t:\n",
    "    for j in ['amend', 'act', 'bill', 'oper', 'implement', 'program', 'titl', \n",
    "              'administration', 'american', 'institut','department', 'secretari', 'offic']:\n",
    "        try:\n",
    "            i.remove(j)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "population_t\n",
    "population_gm = gensim.models.Word2Vec(sentences=population_t, min_count=3, vector_size=300)\n",
    "dfwv = pd.DataFrame(population_gm.wv.vectors)\n",
    "dfwv.index = population_gm.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering based on word vectors\n",
    "optimalK = gap_statistic.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "n_clusters = optimalK(dfwv, cluster_array=np.arange(1, 350))\n",
    "n_clusters\n",
    "optimalK.plot_results()\n",
    "X = population_gm.wv.vectors\n",
    "NUM_CLUSTERS=n_clusters\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "word_clusters = pd.DataFrame({'term':population_gm.wv.index_to_key,'cluster':assigned_clusters})\n",
    "word_clusters.to_csv('word_clusters_edit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of terms to use as single words, outside of clusters\n",
    "single_words = pd.DataFrame({'term':['safeti', 'transit', 'secur', 'effici', 'job', 'vehicl', 'reimburs', 'construct'\n",
    "                                    'research', 'school', 'compet', 'youth', 'young', 'clean', 'production',\n",
    "                                    'power']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a540c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "training_ins = TabularDescriptionTrain(dataset=training_set, text_column='title', word_clusters=word_clusters, \n",
    "                                       single_words=single_words)\n",
    "ts_x, ts_y = training_ins.prepare_set_for_training()\n",
    "training_ins.get_training_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "evaluation_ins = TabularDescriptionTest(dataset=evaluation_set, text_column='title', \n",
    "                                        word_clusters=word_clusters, training_features=training_ins.get_training_features(), \n",
    "                                        single_words=single_words)\n",
    "ev_x, ev_y = evaluation_ins.prepare_set_for_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "test_ins = TabularDescriptionTest(dataset=test_set, text_column='title', \n",
    "                                        word_clusters=word_clusters, training_features=training_ins.get_training_features(), \n",
    "                                        single_words=single_words)\n",
    "test_x, test_y = test_ins.prepare_set_for_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train catboost\n",
    "major_model_80 = catboost.CatBoostClassifier(iterations=18000, max_depth=10, \n",
    "                                             learning_rate=0.025, l2_leaf_reg=0.75, \n",
    "                                             loss_function='MultiClassOneVsAll',\n",
    "                                            rsm=0.2,\n",
    "                                            cat_features=['congress_gov_major_topic']) \n",
    "\n",
    "major_model_80_mt = major_model_80.fit(X=training_80_dtm, y=y_training_80, \n",
    "                                       early_stopping_rounds=50, \n",
    "                                       eval_set=(validation_80_dtm, y_validation_80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shrink number of iterations based on early detection of overfit\n",
    "major_model_80_mt.shrink(9234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "ts_set_acc = major_model_80_mt.score(X=training_80_dtm, y=y_training_80)\n",
    "eval_acc = major_model_80_mt.score(X=validation_80_dtm, y=y_validation_80)\n",
    "test_set_acc = major_model_80_mt.score(X=test_set_80_dtm, y=y_test_set_80)\n",
    "\n",
    "print(f'training set accuracy: {ts_set_acc}')\n",
    "print(f'validation set accuracy: {eval_acc}')\n",
    "print(f'test set accuracy: {test_set_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7929df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "pred = get_classification_results(cbm_model = model, x=test_set_80_dtm, y=y_test_set_80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbp",
   "language": "python",
   "name": "cbp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
